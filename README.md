# Module 3 – Lab 1  
## Distance Metrics & KNN (Getting Geometry to Think)

In this lab, we explore how **distance itself becomes the decision-maker** in machine learning.  
Using **K-Nearest Neighbors (KNN)**, we study how different distance metrics shape decision boundaries, affect accuracy, and break down in high-dimensional spaces.

---

## What This Lab Does

- Builds intuition behind **distance-metric based classification**
- Experiments with **Euclidean, Manhattan, Chebyshev, and Minkowski** distances
- Visualizes how **KNN decision boundaries** change with different values of K
- Demonstrates the **curse of dimensionality** through distance concentration

---

## Why This Matters

KNN doesn’t learn parameters — it learns **geometry**.  
This lab shows how the *choice of distance* can make or break a model.

---

## Tools & Setup

- Python  
- NumPy  
- Matplotlib  
- SciPy  
- scikit-learn  
- Google Colab  

---

## Key Takeaways

- Smaller K → flexible but noisy boundaries  
- Larger K → smoother but less expressive boundaries  
- Distance metrics behave very differently in high dimensions  
- Not all “closeness” is meaningful when dimensions grow  

---

## Module Info

- **Module:** 3  
- **Lab:** 1 of 3  
- **Focus:** Distance Metrics & KNN  

---
